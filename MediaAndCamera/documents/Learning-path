1. 镜头(lens)：
透镜结构，有机玻璃和石英，相当于人眼的晶状体；
按光圈类型分：自动光圈/手动光圈/固定光圈
按变焦类型分：自动变焦/手动变焦/固定焦距
按焦距分：超短焦(广角)、标准镜头、中长焦、长焦
注意，调焦与变焦是两个完全不同的概念。调焦、聚焦、对焦是一个概念，是通过调整
像距以便得到清晰的影像；而变焦是改变镜头的焦距来达到影像清晰的目的。(请恶补凸透镜成像原理)
--此时为光线

2. 图像传感器(sensor)
是一种半导体芯片，有两种类型：ccd和cmos，sensor将从镜头上传导过来的
光线转换为电信号，再通过AD转换将模拟信号量化为数字信号。
--此时为数字信号(raw data格式)

3. ISP/DSP
主要完成数字信号的处理工作。

step1. 首先把sensor采集到的raw data数据映射为色彩格式的数据：yuv或rgb
--此时为yuv或rgb格式数据
yuv与rgb一样都是用来描述像素点颜色的色彩模型，两者可以通过公式进行转换。
rgb(红绿蓝)是从颜色发光的原理出发来设计的一种表征颜色的方式；
常见格式：rgb555、rgb565、rgb24(rgb 888)、rgb32(rgb8888/rgba)
注：rgb后面跟的数字表示rgb三个分量用多少个bit位来表示。
yuv的y表示亮度(灰度值)，uv表示色度
常见格式：yuv444、yuv422、yuv411、yuv420
注：yuv后面的数字分别表示y分量采用的次数、u分量采样的次数、v分量采样的次数
yuv422又可根据yuv三个分量的排列顺序细分为多种格式，如：yuyv、uyvy等

step2. 对图像数据进行前期处理、编码
图像处理器：基本上跟图像效果相关的它都有份，例如：去坏点(去掉sensor中的坏点数据)、
去燥、3A（自动白平衡、自动对焦、自动曝光）、对比度、旋转、锐化、缩放、颜色增强
肤色增强等等
--经过处理后，此时仍然为yuv或rgb格式数据
编码器：通过硬件编码器对图像数据进一步量化(压缩编码)为特定格式以方便存储、传输；
为什么要编码？
我们知道视频可以形象的理解为一系列图片组成，一张图片有一系列像素点组成，每一个像素点由reg或yuv值来表示，但每一张图片肯定会有很多像素点的值是一样的，前后相邻几张
图片的某些区域很可能也是一样的，因此就出现了各种针对图片和视频的压缩(编码)算法。
常见的图片编码算法(标准)：jpg(jpeg)、bmp、png等
常见的视频编码算法(标准)：mjpeg、h264、h265等
这些编码算法中肯定有一些参数是可配置的，因此我们需要了解、学习这些参数的意义、对编码算法的影响、对图像质量的影响。
以学习h264算法为例，以下概念可能需要涉及到，请分别百度之即可理解。
profile、level、VCL层、NAL层、图像、场、帧、宏块、片、I帧、B帧、P帧、GOP、预测编码、变换编码、熵编码、帧内压缩、帧间压缩。
--经过编码器后，此时为经过特定压缩算法后的数据流，通常以帧为单位。

4. 存储/网络传输
拿到了上面经过压缩的数据(或者称编码后的数据)，就可以用来存储以及网络的流式传输了以及其它形式的流式传输。
1）本地存储
需要考虑按照什么样的结构来组织存储呢？如何存储能便于后续的解码播放？于是又形成了一系列的存储标准，例如常见的avi、rmvb、mp4等等
以avi为例，该封装格式包含文件头、数据块、索引块
数据块包含实际的音频数据、视频数据，这里的视频数据就是上面压缩后的以帧为单位的数据。
文件头用来描述音视频数据使用的压缩算法及参数，将来解码、播放该文件时会使用到这些参数。
索引块用来快速定位数据块中的音视频数据。
当然，你可以直接将数据裸写入文件中，这样的话，是无法通过第三方播放器进行解码播放的，因为解码算法和编码算法是对应的，不了解此文件的编码算分细节就无法实现解码。
2）网络传输
网络传输，一般用于实时播放或远程存储，最简单的场景就是将上面的数据以帧为单位直接发送给对方。
但某些场景下对实时性、可靠性要求比较高，例如直播，发送方采集数据、数据编码、接收方需要一边接收一边解码播放，就需要考虑音视频同步、网络传输的延时、不
可靠性造成的影响等等，因此在发送一帧数据时往往要附加一些信息以辅助对方实现获取、解码，纠错等，于是也形成了一系列标准视频传输协议。
例如rtsp、hls等
3）其它形式的流式传输
TS流：传统机顶盒(地面机顶盒DVB-T、卫星机顶盒DVB-S、有线机顶盒DVB-C)接收的音视频节目是ts流
另外PS流、PES流的概念请百度之。